#!/usr/bin/env python3
import os,sys,fnmatch
from optparse import OptionParser,OptionGroup#FIXME: deprecated, use argparse instead!
join=os.path.join
import errno

%%INCLUDED_FUNCTIONS%%

from dgbuild.junit_xml import TestSuite as JU_TestSuite, TestCase as JU_TestCase
import dgbuild.cfg

def perform_tests(testdir,installdir,njobs,prefix,nexcerpts,filters):
    assert not os.path.exists(testdir)
    testdir=os.path.abspath(testdir)
    line_hr = "%s ---------------------------------------+-----------+--------+----------+------------------"%prefix
    line_titles = "%s  Test job results                      | Time [ms] | Job EC | Log-diff | Trouble info"%prefix
    header='\n'.join([line_hr,line_titles,line_hr])
    footer=line_hr

    logdir=join(installdir,'tests/testref')
    bindirs=[join(installdir,'bin'),join(installdir,'scripts')]
    logfiles = set(fn for fn in os.listdir(logdir) if fn.endswith('.log')) if os.path.exists(logdir) else set()

    def ldtest(d):
        if os.path.exists(d):
            for f in os.listdir(d):
                if os.path.isdir(os.path.join(d,f)):
                    continue#skip some weird .DSYM dirs for fortran apps on osx
                if f+'.log' in logfiles or runnable_is_test(f):
                    yield join(d,f)
    tests=set()
    testinfo = {}
    for p in (p for _,p in dgbuild.cfg.pkgs.items() if p['enabled']):
        reflogs, runnables = p['reflogs'],p['runnables']
        for runnable in runnables:
            hasreflog = '%s.log'%runnable in reflogs
            if hasreflog or runnable_is_test(runnable):
                if filters:
                    if not any(fnmatch.fnmatch(runnable,fltr) for fltr in filters):
                        continue
                assert not runnable in tests
                tests.add(runnable)
                testinfo[runnable] = {'pkgname':p['name']}

    if not tests:
        print (header)
        print ('%s  n/a'%prefix)
        print (footer)
        return

    mkdir_p(testdir)
    mf=open(join(testdir,'Makefile'),'w')

    mf.write('TESTDIR:=%s\n\n'%os.path.abspath(testdir))

    mf.write('.DEFAULT: all\n\n')
    mf.write('.PHONY: all\n\n')

    alltests=[]
    for t in tests:
        bn=os.path.basename(t)
        td=join(testdir,bn)
        mkdir_p(td)
        tf=open(join(td,'run.sh'),'w')
        tf.write('#!/bin/bash\n')
        tf.write('mkdir rundir && cd rundir && touch ../time_start && %s &> ../output.log \n'%bn)
        tf.write('EC=$?\n')
        tf.write('touch ../time_end\n')
        tf.write('echo $EC > ../ec.txt\n')
        tf.write('python3 -c \'import os.path;print (1000.0*(os.path.getmtime("../time_end")-os.path.getmtime("../time_start")))\' > ../timing_ms\n')
        if bn+'.log' in logfiles:
            ref=join(installdir,'tests/testref/%s.log'%bn)
            tf.write("""if [ $EC == 0 ]; then
      diff -a --ignore-all-space %s ../output.log > ../refdiff.log || diff -a --ignore-all-space -y %s ../output.log > ../refdiff_sidebyside.log; EC=$?; echo $EC > ../ecdiff.txt
    fi"""%(ref,ref))
        tf.close()

        alltests+=[bn]
        mf.write('.PHONY: %s\n\n'%bn)
        mf.write('%s:\n'%bn)
        mf.write('\t@cd ${TESTDIR}/%s && chmod +x run.sh && ./run.sh'%bn)
        mf.write('\n\n')

    mf.write('all: %s\n\n'%(' '.join(alltests)))
    mf.close()
    os.environ['DISPLAY']=''
    os.environ['PYTHONUNBUFFERED']='1'
    ec_global=system('make -j%i -k -f %s/Makefile all'%(njobs,testdir))
    rep=[]
    for t in alltests:
        td=join(testdir,t)
        ecdiff=None if not os.path.exists(join(td,'ecdiff.txt')) else int(open(join(td,'ecdiff.txt')).read())
        rep+=[(t,int(open(join(td,'ec.txt')).read()),ecdiff)]
    rep.sort()
    excerpts_to_print=[]
    print (header)
    for t,ec,ecdiff in rep:
        time_ms=float(open(join(testdir,t,'timing_ms')).read())
        ecstr='FAIL' if ec else ' OK '
        logdiffstr=' -- '
        if ecdiff is not None:
            logdiffstr='FAIL' if ecdiff else ' OK '
        info='--'
        if ec!=0 or (ecdiff!=None and ecdiff):
            info=os.path.realpath(join(testdir,t))
            ec_global=1
            if nexcerpts>0:
                excerpts_to_print += [(t,testdir,'output.log' if ec!=0 else 'refdiff.log')]
        print ("%s  %-37s |  %6.0f   |  %s  |   %s   | %s"%(prefix,t,time_ms,
                                                           ecstr,
                                                           logdiffstr,info))
        logdiffok = (ecdiff is None or not ecdiff)
        testinfo[t].update( dict(time_ms = time_ms,
                                 exitcode = ec,
                                 logfile = join(testdir,t,'output.log'),
                                 logdiffok = logdiffok,
                                 logdifffile = (join(testdir,t,'refdiff.log') if not logdiffok else None)))
    assert len(alltests)==len(set(alltests))

    print (footer)

    do_junitxml = True
    if do_junitxml:
        test_cases = []
        for testcmd in tests:
            nfo = testinfo[testcmd]
            tc=JU_TestCase(name = testcmd,
                           classname = nfo['pkgname'],
                           elapsed_sec = nfo['time_ms']*0.001)
            def readsafe(fn):
                with open(fn,'rb') as fh:
                    s = fh.read().decode('utf-8','backslashreplace')
                return s if len(s)<11000 else (s[0:5000]+"\n<<<skipping to the ending>>>\n"+ s[-5000:] +"\n<<<output exceeds limit for inclusion here>>>")
            if nfo['exitcode'] != 0:
                tc.add_failure_info("Execution failed",readsafe(nfo['logfile']) or "<command gave no output>")
            elif not nfo['logdiffok']:
                tc.add_failure_info("Output differs from reference",readsafe(nfo['logdifffile']) or "<diff mysteriously missing>")
            test_cases += [tc]
        ts = JU_TestSuite("dgbuild-tests", test_cases)
        outfile = os.path.join(testdir,'dgtest_results_junitformat.xml')
        with open(outfile, 'w') as f:
            JU_TestSuite.to_file(f, [ts], prettyprint=False)
            print('%s\n%s  Test results are also summarised in %s\n%s'%(prefix,prefix,os.path.basename(outfile),prefix))

    if excerpts_to_print:
        for t,testdir,logname in excerpts_to_print:
            print ('\n====>\n====> First %i lines from %s/%s:\n====>'%(nexcerpts,t,logname))
            system('head -%i %s'%(nexcerpts,os.path.join(testdir,t,logname)))
            print ('====> (end of %s/%s)\n'%(t,logname))
            print ('\n====>\n====> Last %i lines from %s/%s:\n====>'%(nexcerpts,t,logname))
            system('tail -%i %s'%(nexcerpts,os.path.join(testdir,t,logname)))
            print ('====> (end of %s/%s)\n'%(t,logname))
    return ec_global

parser = OptionParser(usage='%prog [options]')
parser.add_option("-j", "--jobs",
                  type="int", dest="njobs", default=1,
                  help="use up to N parallel processes",metavar="N")
parser.add_option("-p", "--prefix",type='string',help="prefix output",
                  action='store', dest="prefix", default='')
parser.add_option("-d", "--dir",type='string',help="directory for running tests",
                  action='store', dest="dir", default='')
parser.add_option("-e", "--excerpts",
                  type="int", dest="nexcerpts", default=0,
                  help="Show N first and last lines of each log file in failed tests",metavar="N")
parser.add_option("-f", "--filter",
                  type="str", dest="filters", default='',
                  help="Only run tests with names matching provided patterns (use wildcards and comma separation to specify)",metavar="PATTERN")
(opt, args) = parser.parse_args()
if args:
    parser.error('Unrecognised arguments: %s'%(' '.join(args)))
if not opt.dir:
    fallback=os.path.join(os.getcwd(),'tests')
    if os.path.exists(fallback):
        parser.error('You must supply a desired test directory or be in a directory without a tests/ subdirectory')
    opt.dir=fallback
if os.path.exists(opt.dir):
    parser.error('Test directory must not exist already')

instdir=os.getenv("ESS_INSTALL_PREFIX")
assert instdir and os.path.isdir(instdir)

col_ok = '\033[92m'
col_bad = '\033[91m'
col_end = '\033[0m'

print (opt.prefix+'Running tests in %s:\n'%opt.dir+opt.prefix)

sys.stdout.flush()
sys.stderr.flush()

ec=perform_tests(opt.dir,instdir,njobs=opt.njobs,prefix=opt.prefix,nexcerpts=opt.nexcerpts,
                 filters=[fltr.strip() for fltr in opt.filters.split(',') if fltr.strip()])

sys.stdout.flush()
sys.stderr.flush()

if ec:
    print (opt.prefix+'  %sERROR: Some tests failed!%s'%(col_bad,col_end))
    print (opt.prefix)
    sys.exit(1 if ec > 128 else ec)
else:
    print (opt.prefix+'  %sAll tests completed without failures!%s'%(col_ok,col_end))
    print (opt.prefix)


##
##Help on class TestCase in module dgbuild.junit_xml:
##
##class TestCase(builtins.object)
## |  A JUnit test case with a result and possibly some stdout or stderr
## |
## |  Methods defined here:
## |
## |  __init__(self, name, classname=None, elapsed_sec=None, stdout=None, stderr=None, assertions=None, timestamp=None, status=None, category=None, file=None, line=None, log=None, group=None, url=None)
## |      Initialize self.  See help(type(self)) for accurate signature.
## |
## |  add_error_info(self, message=None, output=None, error_type=None)
## |      Adds an error message, output, or both to the test case
## |
## |  add_failure_info(self, message=None, output=None, failure_type=None)
## |      Adds a failure message, output, or both to the test case
## |
## |  add_skipped_info(self, message=None, output=None)
## |      Adds a skipped message, output, or both to the test case
## |
## |  is_error(self)
## |      returns true if this test case is an error
## |
## |  is_failure(self)
## |      returns true if this test case is a failure
## |
## |  is_skipped(self)
## |      returns true if this test case has been skipped
##
